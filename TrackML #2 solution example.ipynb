{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home_local/vanoli/PycharmProjects/trackparticle\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "print(os.getcwd())\n",
    "prefix='../input/trackml-particle-identification/'\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --user ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "6cfcc4d7929c3744e906026c56c9f8d03a0a46df"
   },
   "outputs": [],
   "source": [
    "def init_model(fs = 10):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2000, activation='selu', input_shape=(fs,)))\n",
    "    model.add(Dense(1000, activation='selu'))\n",
    "    model.add(Dense(1000, activation='selu'))\n",
    "    model.add(Dense(1000, activation='selu'))\n",
    "    model.add(Dense(500, activation='selu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "def get_event(event):\n",
    "    zf = zipfile.ZipFile('train_1.zip')\n",
    "    hits= pd.read_csv(zf.open('train_1/%s-hits.csv'%event))\n",
    "    cells= pd.read_csv(zf.open('train_1/%s-cells.csv'%event))\n",
    "    truth= pd.read_csv(zf.open('train_1/%s-truth.csv'%event))\n",
    "    particles= pd.read_csv(zf.open('train_1/%s-particles.csv'%event))\n",
    "    return hits, cells, truth, particles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22ca78487ca459c532750dd133c7ed401e592e3d"
   },
   "source": [
    "# Step 1 - Prepare training data\n",
    "* use 10 events for training\n",
    "* input: hit pair\n",
    "* output: 1 if two hits are the same particle_id, 0 otherwise.\n",
    "* feature size: 10 (5 per hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "8a8aa70428d754645a5ab2c74f7bc6c3c99505b7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd822afa243d42d9a84d3cc0d8b08b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event000001010 (905342, 11)\n",
      "event000001011 (1049940, 11)\n",
      "event000001012 (975962, 11)\n",
      "event000001013 (937140, 11)\n",
      "event000001014 (1138386, 11)\n",
      "event000001015 (1096946, 11)\n",
      "event000001016 (1054472, 11)\n",
      "event000001017 (1125976, 11)\n",
      "event000001018 (787588, 11)\n",
      "event000001019 (1099946, 11)\n",
      "\n",
      "(40684241, 11)\n"
     ]
    }
   ],
   "source": [
    "# you can jump to step4 for test only.\n",
    "train = True\n",
    "if train:\n",
    "    Train = []\n",
    "    for i in tqdm(range(10,20)):\n",
    "        event = 'event0000010%02d'%i\n",
    "        hits, cells, truth, particles = get_event(event)\n",
    "        hit_cells = cells.groupby(['hit_id']).value.count().values\n",
    "        hit_value = cells.groupby(['hit_id']).value.sum().values\n",
    "        features = np.hstack((hits[['x','y','z']]/1000, hit_cells.reshape(len(hit_cells),1)/10,hit_value.reshape(len(hit_cells),1)))\n",
    "        particle_ids = truth.particle_id.unique()\n",
    "        particle_ids = particle_ids[np.where(particle_ids!=0)[0]]\n",
    "\n",
    "        pair = []\n",
    "        for particle_id in particle_ids:\n",
    "            hit_ids = truth[truth.particle_id == particle_id].hit_id.values-1\n",
    "            for i in hit_ids:\n",
    "                for j in hit_ids:\n",
    "                    if i != j:\n",
    "                        pair.append([i,j])\n",
    "        pair = np.array(pair)   \n",
    "        Train1 = np.hstack((features[pair[:,0]], features[pair[:,1]], np.ones((len(pair),1))))\n",
    "\n",
    "        if len(Train) == 0:\n",
    "            Train = Train1\n",
    "        else:\n",
    "            Train = np.vstack((Train,Train1))\n",
    "\n",
    "        n = len(hits)\n",
    "        size = len(Train1)*3\n",
    "        p_id = truth.particle_id.values\n",
    "        i =np.random.randint(n, size=size)\n",
    "        j =np.random.randint(n, size=size)\n",
    "        pair = np.hstack((i.reshape(size,1),j.reshape(size,1)))\n",
    "        pair = pair[((p_id[i]==0) | (p_id[i]!=p_id[j]))]\n",
    "\n",
    "        Train0 = np.hstack((features[pair[:,0]], features[pair[:,1]], np.zeros((len(pair),1))))\n",
    "\n",
    "        print(event, Train1.shape)\n",
    "\n",
    "        Train = np.vstack((Train,Train0))\n",
    "    del Train0, Train1\n",
    "\n",
    "    np.random.shuffle(Train)\n",
    "    print(Train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f38fbd32829f9ca2a18f8b00bf3db26f57f4297"
   },
   "source": [
    "# Step 2 - Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "3bab2db55c8d0abf38119e4f6780269adaaf906e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 800)               8800      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 400)               320400    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 400)               160400    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 730,401\n",
      "Trainable params: 730,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    model = init_model()\n",
    "    model.summary()\n",
    "    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='model.weights.best.hdf5', \n",
    "                                                  verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "9e96d105f22f0fb6d605332c43994b7608744bcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4832/4832 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.8643\n",
      "Epoch 00001: val_loss improved from inf to 0.20904, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 38s 8ms/step - loss: 0.2938 - accuracy: 0.8643 - val_loss: 0.2090 - val_accuracy: 0.9121\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    lr=-5\n",
    "    model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=10**(lr)), metrics=['accuracy'])\n",
    "    History = model.fit(x=Train[:,:-1], y=Train[:,-1], batch_size=8000, epochs=1, verbose=1, callbacks=checkpointer, validation_split=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "29eb9b3b2db4909cf9993435075bf0561b2cc468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4828/4832 [============================>.] - ETA: 0s - loss: 0.1664 - accuracy: 0.9348\n",
      "Epoch 00001: val_loss improved from 0.20904 to 0.14885, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 37s 8ms/step - loss: 0.1664 - accuracy: 0.9348 - val_loss: 0.1488 - val_accuracy: 0.9424\n",
      "Epoch 2/20\n",
      "4831/4832 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9547\n",
      "Epoch 00002: val_loss improved from 0.14885 to 0.11678, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 37s 8ms/step - loss: 0.1261 - accuracy: 0.9547 - val_loss: 0.1168 - val_accuracy: 0.9581\n",
      "Epoch 3/20\n",
      "4829/4832 [============================>.] - ETA: 0s - loss: 0.1124 - accuracy: 0.9611\n",
      "Epoch 00003: val_loss improved from 0.11678 to 0.10049, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 37s 8ms/step - loss: 0.1124 - accuracy: 0.9611 - val_loss: 0.1005 - val_accuracy: 0.9669\n",
      "Epoch 4/20\n",
      "4829/4832 [============================>.] - ETA: 0s - loss: 0.1030 - accuracy: 0.9651\n",
      "Epoch 00004: val_loss did not improve from 0.10049\n",
      "4832/4832 [==============================] - 37s 8ms/step - loss: 0.1030 - accuracy: 0.9651 - val_loss: 0.1076 - val_accuracy: 0.9613\n",
      "Epoch 5/20\n",
      "4827/4832 [============================>.] - ETA: 0s - loss: 0.0978 - accuracy: 0.9674\n",
      "Epoch 00005: val_loss improved from 0.10049 to 0.09241, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 37s 8ms/step - loss: 0.0978 - accuracy: 0.9674 - val_loss: 0.0924 - val_accuracy: 0.9691\n",
      "Epoch 6/20\n",
      "4828/4832 [============================>.] - ETA: 0s - loss: 0.0919 - accuracy: 0.9695\n",
      "Epoch 00006: val_loss improved from 0.09241 to 0.08649, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 38s 8ms/step - loss: 0.0919 - accuracy: 0.9695 - val_loss: 0.0865 - val_accuracy: 0.9714\n",
      "Epoch 7/20\n",
      "4829/4832 [============================>.] - ETA: 0s - loss: 0.0898 - accuracy: 0.9705\n",
      "Epoch 00007: val_loss improved from 0.08649 to 0.08291, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 38s 8ms/step - loss: 0.0898 - accuracy: 0.9705 - val_loss: 0.0829 - val_accuracy: 0.9735\n",
      "Epoch 8/20\n",
      "4826/4832 [============================>.] - ETA: 0s - loss: 0.0849 - accuracy: 0.9722\n",
      "Epoch 00008: val_loss improved from 0.08291 to 0.08110, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 39s 8ms/step - loss: 0.0849 - accuracy: 0.9722 - val_loss: 0.0811 - val_accuracy: 0.9735\n",
      "Epoch 9/20\n",
      "4825/4832 [============================>.] - ETA: 0s - loss: 0.0815 - accuracy: 0.9734\n",
      "Epoch 00009: val_loss did not improve from 0.08110\n",
      "4832/4832 [==============================] - 37s 8ms/step - loss: 0.0815 - accuracy: 0.9734 - val_loss: 0.0813 - val_accuracy: 0.9723\n",
      "Epoch 10/20\n",
      "4831/4832 [============================>.] - ETA: 0s - loss: 0.0793 - accuracy: 0.9742\n",
      "Epoch 00010: val_loss improved from 0.08110 to 0.07841, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 38s 8ms/step - loss: 0.0793 - accuracy: 0.9742 - val_loss: 0.0784 - val_accuracy: 0.9746\n",
      "Epoch 11/20\n",
      "4829/4832 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9754\n",
      "Epoch 00011: val_loss improved from 0.07841 to 0.07358, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 38s 8ms/step - loss: 0.0754 - accuracy: 0.9754 - val_loss: 0.0736 - val_accuracy: 0.9757\n",
      "Epoch 12/20\n",
      "4826/4832 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9762\n",
      "Epoch 00012: val_loss did not improve from 0.07358\n",
      "4832/4832 [==============================] - 38s 8ms/step - loss: 0.0732 - accuracy: 0.9762 - val_loss: 0.0747 - val_accuracy: 0.9751\n",
      "Epoch 13/20\n",
      "4831/4832 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9768\n",
      "Epoch 00013: val_loss improved from 0.07358 to 0.06797, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 38s 8ms/step - loss: 0.0713 - accuracy: 0.9768 - val_loss: 0.0680 - val_accuracy: 0.9788\n",
      "Epoch 14/20\n",
      "4827/4832 [============================>.] - ETA: 0s - loss: 0.0690 - accuracy: 0.9777\n",
      "Epoch 00014: val_loss improved from 0.06797 to 0.06673, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 38s 8ms/step - loss: 0.0690 - accuracy: 0.9777 - val_loss: 0.0667 - val_accuracy: 0.9787\n",
      "Epoch 15/20\n",
      "4830/4832 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.9784\n",
      "Epoch 00015: val_loss did not improve from 0.06673\n",
      "4832/4832 [==============================] - 38s 8ms/step - loss: 0.0668 - accuracy: 0.9784 - val_loss: 0.0671 - val_accuracy: 0.9785\n",
      "Epoch 16/20\n",
      "4832/4832 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9790\n",
      "Epoch 00016: val_loss did not improve from 0.06673\n",
      "4832/4832 [==============================] - 37s 8ms/step - loss: 0.0649 - accuracy: 0.9790 - val_loss: 0.0676 - val_accuracy: 0.9779\n",
      "Epoch 17/20\n",
      "4828/4832 [============================>.] - ETA: 0s - loss: 0.0633 - accuracy: 0.9795\n",
      "Epoch 00017: val_loss improved from 0.06673 to 0.06237, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 37s 8ms/step - loss: 0.0633 - accuracy: 0.9795 - val_loss: 0.0624 - val_accuracy: 0.9792\n",
      "Epoch 18/20\n",
      "4826/4832 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9801\n",
      "Epoch 00018: val_loss did not improve from 0.06237\n",
      "4832/4832 [==============================] - 38s 8ms/step - loss: 0.0615 - accuracy: 0.9801 - val_loss: 0.0625 - val_accuracy: 0.9793\n",
      "Epoch 19/20\n",
      "4828/4832 [============================>.] - ETA: 0s - loss: 0.0600 - accuracy: 0.9806\n",
      "Epoch 00019: val_loss improved from 0.06237 to 0.05998, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 37s 8ms/step - loss: 0.0599 - accuracy: 0.9806 - val_loss: 0.0600 - val_accuracy: 0.9806\n",
      "Epoch 20/20\n",
      "4829/4832 [============================>.] - ETA: 0s - loss: 0.0583 - accuracy: 0.9811\n",
      "Epoch 00020: val_loss improved from 0.05998 to 0.05823, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 37s 8ms/step - loss: 0.0583 - accuracy: 0.9811 - val_loss: 0.0582 - val_accuracy: 0.9813\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    lr=-4\n",
    "    model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=10**(lr)), metrics=['accuracy'])\n",
    "    History = model.fit(x=Train[:,:-1], y=Train[:,-1], batch_size=8000, epochs=20, verbose=1, callbacks=checkpointer, validation_split=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "ab41ee83a17a170f907b0789379420cee8da037f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4831/4832 [============================>.] - ETA: 0s - loss: 0.0530 - accuracy: 0.9829\n",
      "Epoch 00001: val_loss improved from 0.05823 to 0.05329, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 37s 8ms/step - loss: 0.0530 - accuracy: 0.9829 - val_loss: 0.0533 - val_accuracy: 0.9827\n",
      "Epoch 2/3\n",
      "4826/4832 [============================>.] - ETA: 0s - loss: 0.0526 - accuracy: 0.9830\n",
      "Epoch 00002: val_loss improved from 0.05329 to 0.05287, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 37s 8ms/step - loss: 0.0526 - accuracy: 0.9830 - val_loss: 0.0529 - val_accuracy: 0.9829\n",
      "Epoch 3/3\n",
      "4827/4832 [============================>.] - ETA: 0s - loss: 0.0522 - accuracy: 0.9831\n",
      "Epoch 00003: val_loss improved from 0.05287 to 0.05255, saving model to model.weights.best.hdf5\n",
      "4832/4832 [==============================] - 37s 8ms/step - loss: 0.0522 - accuracy: 0.9831 - val_loss: 0.0525 - val_accuracy: 0.9830\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    lr=-5\n",
    "    model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=10**(lr)), metrics=['accuracy'])\n",
    "    History = model.fit(x=Train[:,:-1], y=Train[:,-1], batch_size=8000, epochs=3, verbose=1, callbacks=checkpointer, validation_split=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "185872dc6a27b54f7b810c66a5076796c8c8ec01"
   },
   "source": [
    "# Step 3 - Hard Negative Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "20a591651544390c1940aedd8c48a4f1526e1f8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home_local/vanoli/PycharmProjects/trackparticle\n"
     ]
    }
   ],
   "source": [
    "# if you skip step2, you still need to run step1 to get training data.\n",
    "# if train:\n",
    "#     try:\n",
    "#         model\n",
    "#     except NameError:\n",
    "#         print('load model')\n",
    "print(os.getcwd())\n",
    "model = load_model('model.weights.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "64ce296b3f18f0b4dd368ee232bb9aaa7655365f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0dff00deaf4633819e42cf965ddefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event000001010 29997262 428433\n",
      "event000001011 29997442 417690\n",
      "event000001012 29997364 410492\n",
      "event000001013 29997236 413676\n",
      "event000001014 29997665 417784\n",
      "event000001015 29997578 421373\n",
      "event000001016 29997564 408799\n",
      "event000001017 29997613 413313\n",
      "event000001018 29997009 423236\n",
      "event000001019 29997557 417062\n",
      "\n",
      "(4171858, 11)\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    Train_hard = []\n",
    "\n",
    "    for i in tqdm(range(10,20)):\n",
    "\n",
    "        event = 'event0000010%02d'%i\n",
    "        hits, cells, truth, particles = get_event(event)\n",
    "        hit_cells = cells.groupby(['hit_id']).value.count().values\n",
    "        hit_value = cells.groupby(['hit_id']).value.sum().values\n",
    "        features = np.hstack((hits[['x','y','z']]/1000, hit_cells.reshape(len(hit_cells),1)/10,hit_value.reshape(len(hit_cells),1)))\n",
    "\n",
    "        size=30000000\n",
    "        n = len(truth)\n",
    "        i =np.random.randint(n, size=size)\n",
    "        j =np.random.randint(n, size=size)\n",
    "        p_id = truth.particle_id.values\n",
    "        pair = np.hstack((i.reshape(size,1),j.reshape(size,1)))\n",
    "        pair = pair[((p_id[i]==0) | (p_id[i]!=p_id[j]))]\n",
    "\n",
    "        Train0 = np.hstack((features[pair[:,0]], features[pair[:,1]], np.zeros((len(pair),1))))\n",
    "\n",
    "        pred = model.predict(Train0[:,:-1], batch_size=20000)\n",
    "        s = np.where(pred>0.5)[0]\n",
    "\n",
    "        print(event, len(Train0), len(s))\n",
    "\n",
    "        if len(Train_hard) == 0:\n",
    "            Train_hard = Train0[s]\n",
    "        else:\n",
    "            Train_hard = np.vstack((Train_hard,Train0[s]))\n",
    "    del Train0\n",
    "    print(Train_hard.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "72e5d14bd8617a4e3c5175cea7705cdabcaa0f5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44856099, 11)\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    Train = np.vstack((Train,Train_hard))\n",
    "    np.random.shuffle(Train)\n",
    "    print(Train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "ba695b07a4d35af53127564ba11b7884d56fd1a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5324/5327 [============================>.] - ETA: 0s - loss: 0.1469 - accuracy: 0.9405\n",
      "Epoch 00001: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 42s 8ms/step - loss: 0.1469 - accuracy: 0.9405 - val_loss: 0.1468 - val_accuracy: 0.9408\n",
      "Epoch 2/30\n",
      "5325/5327 [============================>.] - ETA: 0s - loss: 0.1409 - accuracy: 0.9438\n",
      "Epoch 00002: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.1409 - accuracy: 0.9438 - val_loss: 0.1441 - val_accuracy: 0.9423\n",
      "Epoch 3/30\n",
      "5325/5327 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.9465\n",
      "Epoch 00003: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.1359 - accuracy: 0.9465 - val_loss: 0.1311 - val_accuracy: 0.9484\n",
      "Epoch 4/30\n",
      "5324/5327 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9488\n",
      "Epoch 00004: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 42s 8ms/step - loss: 0.1313 - accuracy: 0.9488 - val_loss: 0.1282 - val_accuracy: 0.9502\n",
      "Epoch 5/30\n",
      "5322/5327 [============================>.] - ETA: 0s - loss: 0.1269 - accuracy: 0.9510\n",
      "Epoch 00005: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.1269 - accuracy: 0.9510 - val_loss: 0.1243 - val_accuracy: 0.9521\n",
      "Epoch 6/30\n",
      "5323/5327 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9528\n",
      "Epoch 00006: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.1230 - accuracy: 0.9528 - val_loss: 0.1225 - val_accuracy: 0.9530\n",
      "Epoch 7/30\n",
      "5326/5327 [============================>.] - ETA: 0s - loss: 0.1195 - accuracy: 0.9544\n",
      "Epoch 00007: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.1195 - accuracy: 0.9544 - val_loss: 0.1205 - val_accuracy: 0.9534\n",
      "Epoch 8/30\n",
      "5325/5327 [============================>.] - ETA: 0s - loss: 0.1163 - accuracy: 0.9559\n",
      "Epoch 00008: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.1163 - accuracy: 0.9559 - val_loss: 0.1175 - val_accuracy: 0.9556\n",
      "Epoch 9/30\n",
      "5325/5327 [============================>.] - ETA: 0s - loss: 0.1132 - accuracy: 0.9573\n",
      "Epoch 00009: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.1132 - accuracy: 0.9573 - val_loss: 0.1097 - val_accuracy: 0.9585\n",
      "Epoch 10/30\n",
      "5323/5327 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9584\n",
      "Epoch 00010: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.1106 - accuracy: 0.9584 - val_loss: 0.1081 - val_accuracy: 0.9592\n",
      "Epoch 11/30\n",
      "5326/5327 [============================>.] - ETA: 0s - loss: 0.1078 - accuracy: 0.9596\n",
      "Epoch 00011: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.1078 - accuracy: 0.9596 - val_loss: 0.1074 - val_accuracy: 0.9600\n",
      "Epoch 12/30\n",
      "5322/5327 [============================>.] - ETA: 0s - loss: 0.1053 - accuracy: 0.9606\n",
      "Epoch 00012: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.1053 - accuracy: 0.9606 - val_loss: 0.1070 - val_accuracy: 0.9604\n",
      "Epoch 13/30\n",
      "5322/5327 [============================>.] - ETA: 0s - loss: 0.1030 - accuracy: 0.9617\n",
      "Epoch 00013: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.1030 - accuracy: 0.9617 - val_loss: 0.1028 - val_accuracy: 0.9613\n",
      "Epoch 14/30\n",
      "5325/5327 [============================>.] - ETA: 0s - loss: 0.1008 - accuracy: 0.9626\n",
      "Epoch 00014: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 42s 8ms/step - loss: 0.1008 - accuracy: 0.9626 - val_loss: 0.0998 - val_accuracy: 0.9627\n",
      "Epoch 15/30\n",
      "5322/5327 [============================>.] - ETA: 0s - loss: 0.0986 - accuracy: 0.9635\n",
      "Epoch 00015: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0986 - accuracy: 0.9635 - val_loss: 0.0968 - val_accuracy: 0.9645\n",
      "Epoch 16/30\n",
      "5325/5327 [============================>.] - ETA: 0s - loss: 0.0967 - accuracy: 0.9642\n",
      "Epoch 00016: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0967 - accuracy: 0.9642 - val_loss: 0.0951 - val_accuracy: 0.9648\n",
      "Epoch 17/30\n",
      "5322/5327 [============================>.] - ETA: 0s - loss: 0.0948 - accuracy: 0.9650\n",
      "Epoch 00017: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0948 - accuracy: 0.9650 - val_loss: 0.0948 - val_accuracy: 0.9654\n",
      "Epoch 18/30\n",
      "5326/5327 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9657\n",
      "Epoch 00018: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0930 - accuracy: 0.9657 - val_loss: 0.0921 - val_accuracy: 0.9661\n",
      "Epoch 19/30\n",
      "5325/5327 [============================>.] - ETA: 0s - loss: 0.0914 - accuracy: 0.9664\n",
      "Epoch 00019: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0914 - accuracy: 0.9664 - val_loss: 0.0922 - val_accuracy: 0.9658\n",
      "Epoch 20/30\n",
      "5323/5327 [============================>.] - ETA: 0s - loss: 0.0898 - accuracy: 0.9670\n",
      "Epoch 00020: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0898 - accuracy: 0.9670 - val_loss: 0.0904 - val_accuracy: 0.9668\n",
      "Epoch 21/30\n",
      "5322/5327 [============================>.] - ETA: 0s - loss: 0.0884 - accuracy: 0.9676\n",
      "Epoch 00021: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0884 - accuracy: 0.9676 - val_loss: 0.0867 - val_accuracy: 0.9685\n",
      "Epoch 22/30\n",
      "5323/5327 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9681\n",
      "Epoch 00022: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0870 - accuracy: 0.9681 - val_loss: 0.0878 - val_accuracy: 0.9683\n",
      "Epoch 23/30\n",
      "5326/5327 [============================>.] - ETA: 0s - loss: 0.0857 - accuracy: 0.9686\n",
      "Epoch 00023: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 42s 8ms/step - loss: 0.0857 - accuracy: 0.9686 - val_loss: 0.0863 - val_accuracy: 0.9681\n",
      "Epoch 24/30\n",
      "5321/5327 [============================>.] - ETA: 0s - loss: 0.0845 - accuracy: 0.9691\n",
      "Epoch 00024: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0845 - accuracy: 0.9691 - val_loss: 0.0844 - val_accuracy: 0.9691\n",
      "Epoch 25/30\n",
      "5327/5327 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9695\n",
      "Epoch 00025: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0834 - accuracy: 0.9695 - val_loss: 0.0847 - val_accuracy: 0.9691\n",
      "Epoch 26/30\n",
      "5323/5327 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9699\n",
      "Epoch 00026: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 42s 8ms/step - loss: 0.0823 - accuracy: 0.9699 - val_loss: 0.0830 - val_accuracy: 0.9694\n",
      "Epoch 27/30\n",
      "5322/5327 [============================>.] - ETA: 0s - loss: 0.0813 - accuracy: 0.9703\n",
      "Epoch 00027: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0813 - accuracy: 0.9703 - val_loss: 0.0823 - val_accuracy: 0.9700\n",
      "Epoch 28/30\n",
      "5326/5327 [============================>.] - ETA: 0s - loss: 0.0803 - accuracy: 0.9707\n",
      "Epoch 00028: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0803 - accuracy: 0.9707 - val_loss: 0.0804 - val_accuracy: 0.9708\n",
      "Epoch 29/30\n",
      "5323/5327 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9710\n",
      "Epoch 00029: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0795 - accuracy: 0.9710 - val_loss: 0.0800 - val_accuracy: 0.9708\n",
      "Epoch 30/30\n",
      "5325/5327 [============================>.] - ETA: 0s - loss: 0.0786 - accuracy: 0.9714\n",
      "Epoch 00030: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0786 - accuracy: 0.9714 - val_loss: 0.0788 - val_accuracy: 0.9710\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    lr=-4\n",
    "    model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=10**(lr)), metrics=['accuracy'])\n",
    "    History = model.fit(x=Train[:,:-1], y=Train[:,-1], batch_size=8000, epochs=30, verbose=1,callbacks=checkpointer, validation_split=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "ffecdec926f9645fe4ddac507787980fbf6142a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5323/5327 [============================>.] - ETA: 0s - loss: 0.0726 - accuracy: 0.9738\n",
      "Epoch 00001: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0726 - accuracy: 0.9738 - val_loss: 0.0734 - val_accuracy: 0.9735\n",
      "Epoch 2/10\n",
      "5320/5327 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9739\n",
      "Epoch 00002: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0722 - accuracy: 0.9739 - val_loss: 0.0732 - val_accuracy: 0.9736\n",
      "Epoch 3/10\n",
      "5321/5327 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9740\n",
      "Epoch 00003: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 42s 8ms/step - loss: 0.0721 - accuracy: 0.9740 - val_loss: 0.0730 - val_accuracy: 0.9736\n",
      "Epoch 4/10\n",
      "5324/5327 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9740\n",
      "Epoch 00004: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0719 - accuracy: 0.9740 - val_loss: 0.0730 - val_accuracy: 0.9737\n",
      "Epoch 5/10\n",
      "5321/5327 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9741\n",
      "Epoch 00005: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 42s 8ms/step - loss: 0.0718 - accuracy: 0.9741 - val_loss: 0.0728 - val_accuracy: 0.9738\n",
      "Epoch 6/10\n",
      "5327/5327 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9741\n",
      "Epoch 00006: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0716 - accuracy: 0.9741 - val_loss: 0.0727 - val_accuracy: 0.9737\n",
      "Epoch 7/10\n",
      "5326/5327 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9742\n",
      "Epoch 00007: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0715 - accuracy: 0.9742 - val_loss: 0.0727 - val_accuracy: 0.9737\n",
      "Epoch 8/10\n",
      "5321/5327 [============================>.] - ETA: 0s - loss: 0.0714 - accuracy: 0.9742\n",
      "Epoch 00008: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0714 - accuracy: 0.9742 - val_loss: 0.0725 - val_accuracy: 0.9739\n",
      "Epoch 9/10\n",
      "5323/5327 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9743\n",
      "Epoch 00009: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0713 - accuracy: 0.9743 - val_loss: 0.0726 - val_accuracy: 0.9737\n",
      "Epoch 10/10\n",
      "5326/5327 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9743\n",
      "Epoch 00010: val_loss did not improve from 0.05255\n",
      "5327/5327 [==============================] - 41s 8ms/step - loss: 0.0711 - accuracy: 0.9743 - val_loss: 0.0723 - val_accuracy: 0.9739\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    lr=-5\n",
    "    model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=10**(lr)), metrics=['accuracy'])\n",
    "    History = model.fit(x=Train[:,:-1], y=Train[:,-1], batch_size=8000, epochs=10, verbose=1,callbacks=checkpointer, validation_split=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b81f5b8a3e882fae96bc923f79720ef3f130ca5d"
   },
   "outputs": [],
   "source": [
    "if train:\n",
    "    lr=-6\n",
    "    model.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=10**(lr)), metrics=['accuracy'])\n",
    "    History = model.fit(x=Train[:,:-1], y=Train[:,-1], batch_size=8000, epochs=2, verbose=1,callbacks=checkpointer, validation_split=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e5db9b7e283f01e8016e919c0e4d4c4731bf247"
   },
   "source": [
    "# Step 4 - Test event 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "ba7f35bbd13b606908207e4dbea86190a681f8cc"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    print('load model')\n",
    "    model = load_model('../input/trackml/my_model_h.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b87b9218f8426fc7b86d09156d5d461713993e8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17018\n",
      "0\n",
      "21\n",
      "25\n",
      "32\n",
      "55\n",
      "62\n",
      "79\n",
      "85\n",
      "94\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "event = 'event000001001'\n",
    "hits, cells, truth, particles = get_event(event)\n",
    "hit_cells = cells.groupby(['hit_id']).value.count().values\n",
    "hit_value = cells.groupby(['hit_id']).value.sum().values\n",
    "features = np.hstack((hits[['x','y','z']]/1000, hit_cells.reshape(len(hit_cells),1)/10,hit_value.reshape(len(hit_cells),1)))\n",
    "count = hits.groupby(['volume_id','layer_id','module_id'])['hit_id'].count().values\n",
    "module_id = np.zeros(len(hits), dtype='int32')\n",
    "print(len(count))\n",
    "for i in range(len(count)):\n",
    "    si = np.sum(count[:i])\n",
    "    if i < 10:\n",
    "        print(si)\n",
    "    module_id[si:si+count[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "a= 12\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "f882457cf0e251f4e8e8adce0629d381063a3ca2"
   },
   "outputs": [],
   "source": [
    "def get_path(hit, mask, thr):\n",
    "    path = [hit]\n",
    "    a = 0\n",
    "    while True:\n",
    "        c = get_predict(path[-1], thr/2)\n",
    "        mask = (c > thr)*mask\n",
    "        mask[path[-1]] = 0\n",
    "        \n",
    "        if 1:\n",
    "            cand = np.where(c>thr)[0]\n",
    "            if len(cand)>0:\n",
    "                mask[cand[np.isin(module_id[cand], module_id[path])]]=0\n",
    "                \n",
    "        a = (c + a)*mask\n",
    "        if a.max() < thr*len(path):\n",
    "            break\n",
    "        path.append(a.argmax())\n",
    "    return path\n",
    "\n",
    "def get_predict(hit, thr=0.5):\n",
    "    Tx = np.zeros((len(truth),10))\n",
    "    Tx[:,5:] = features\n",
    "    Tx[:,:5] = np.tile(features[hit], (len(Tx), 1))\n",
    "    pred = model.predict(Tx, batch_size=len(Tx))[:,0]\n",
    "    # TTA\n",
    "    idx = np.where(pred > thr)[0]\n",
    "    Tx2 = np.zeros((len(idx),10))\n",
    "    Tx2[:,5:] = Tx[idx,:5]\n",
    "    Tx2[:,:5] = Tx[idx,5:]    \n",
    "    pred1 = model.predict(Tx2, batch_size=len(idx))[:,0]\n",
    "    pred[idx] = (pred[idx] + pred1)/2\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "98400d020f1339b0a467047d3acff43925f39080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit_id =  1\n",
      "reconstruct : [0, 2665, 1259, 1236, 2720, 4429, 4477, 6494, 6584, 8937, 6556, 9011, 9023, 11352]\n",
      "ground truth: [0, 1236, 1259, 2656, 2711, 4429, 4501, 6492, 6568, 6624, 8927, 9006, 9024, 11361]\n",
      "hit_id =  2\n",
      "reconstruct : [1, 1314, 1270, 2763, 2703, 4546, 4478, 6482, 8917, 6527, 6607, 9038, 11366, 9003, 40]\n",
      "ground truth: [1, 1270, 1314, 2703, 2763, 4478, 4546, 6482, 6527, 6607, 8917, 9003, 9038, 11366]\n",
      "hit_id =  3\n",
      "reconstruct : [2, 1258, 1296, 4437, 2697, 2766, 4536, 4494, 6485, 6619, 6564, 8908, 2667]\n",
      "ground truth: [2, 1258, 2709, 2756, 4428, 4480, 4529, 6480, 6541, 6608, 8925]\n"
     ]
    }
   ],
   "source": [
    "# select one hit to construct a track\n",
    "for hit in range(3):\n",
    "    path = get_path(hit, np.ones(len(truth)), 0.95)\n",
    "    gt = np.where(truth.particle_id==truth.particle_id[hit])[0]\n",
    "    print('hit_id = ', hit+1)\n",
    "    print('reconstruct :', path)\n",
    "    print('ground truth:', gt.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b218782577435528ac3e131eac33e7d4b8f64d75"
   },
   "source": [
    "# Step 5 - Predict and Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "6dd697c803477e88b7d0bd98b24e5c3c649898ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load predicts\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Predict all pairs for reconstruct by all hits. (takes 2.5hr but can skip)\n",
    "skip_predict = True\n",
    "\n",
    "if skip_predict == False:\n",
    "    TestX = np.zeros((len(features), 10))\n",
    "    TestX[:,5:] = features\n",
    "\n",
    "    # for TTA\n",
    "    TestX1 = np.zeros((len(features), 10))\n",
    "    TestX1[:,:5] = features\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for i in tqdm_notebook(range(len(features)-1)):\n",
    "        TestX[i+1:,:5] = np.tile(features[i], (len(TestX)-i-1, 1))\n",
    "\n",
    "        pred = model.predict(TestX[i+1:], batch_size=20000)[:,0]                \n",
    "        idx = np.where(pred>0.2)[0]\n",
    "\n",
    "        if len(idx) > 0:\n",
    "            TestX1[idx+i+1,5:] = TestX[idx+i+1,:5]\n",
    "            pred1 = model.predict(TestX1[idx+i+1], batch_size=20000)[:,0]\n",
    "            pred[idx] = (pred[idx]+pred1)/2\n",
    "\n",
    "        idx = np.where(pred>0.5)[0]\n",
    "\n",
    "        preds.append([idx+i+1, pred[idx]])\n",
    "\n",
    "        #if i==0: print(preds[-1])\n",
    "\n",
    "    preds.append([np.array([], dtype='int64'), np.array([], dtype='float32')])\n",
    "\n",
    "    # rebuild to NxN\n",
    "    for i in range(len(preds)):\n",
    "        ii = len(preds)-i-1\n",
    "        for j in range(len(preds[ii][0])):\n",
    "            jj = preds[ii][0][j]\n",
    "            preds[jj][0] = np.insert(preds[jj][0], 0 ,ii)\n",
    "            preds[jj][1] = np.insert(preds[jj][1], 0 ,preds[ii][1][j])\n",
    "\n",
    "    np.save('my_%s.npy'%event, preds)\n",
    "else:\n",
    "    print('load predicts')\n",
    "    preds = np.load('my_%s.npy'%event)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "0d34e0bcdc6ba971b4bc7bab1e8c185e17523956"
   },
   "outputs": [],
   "source": [
    "def get_path2(hit, mask, thr):\n",
    "    path = [hit]\n",
    "    a = 0\n",
    "    while True:\n",
    "        c = get_predict2(path[-1])\n",
    "        mask = (c > thr)*mask\n",
    "        mask[path[-1]] = 0\n",
    "        \n",
    "        if 1:\n",
    "            cand = np.where(c>thr)[0]\n",
    "            if len(cand)>0:\n",
    "                mask[cand[np.isin(module_id[cand], module_id[path])]]=0\n",
    "                \n",
    "        a = (c + a)*mask\n",
    "        if a.max() < thr*len(path):\n",
    "            break\n",
    "        path.append(a.argmax())\n",
    "    return path\n",
    "\n",
    "def get_predict2(p):\n",
    "    c = np.zeros(len(preds))\n",
    "    c[preds[p, 0]] = preds[p, 1]          \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "0a1af92b508896ec7fc6da0d5f89f153246f73b1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74dd5b5a92514f638bf219c5e0bc9e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=93680), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# reconstruct by all hits. (takes 0.6hr but can skip)\n",
    "skip_reconstruct = False\n",
    "\n",
    "if skip_reconstruct == False:\n",
    "    tracks_all = []\n",
    "    thr = 0.85\n",
    "    x4 = True\n",
    "    for hit in tqdm(range(len(preds))):\n",
    "        m = np.ones(len(truth))\n",
    "        path  = get_path2(hit, m, thr)\n",
    "        if x4 and len(path) > 1:\n",
    "            m[path[1]]=0\n",
    "            path2  = get_path2(hit, m, thr)\n",
    "            if len(path) < len(path2):\n",
    "                path = path2\n",
    "                m[path[1]]=0\n",
    "                path2  = get_path2(hit, m, thr)\n",
    "                if len(path) < len(path2):\n",
    "                    path = path2\n",
    "            elif len(path2) > 1:\n",
    "                m[path[1]]=1\n",
    "                m[path2[1]]=0\n",
    "                path2  = get_path2(hit, m, thr)\n",
    "                if len(path) < len(path2):\n",
    "                    path = path2\n",
    "        tracks_all.append(path)\n",
    "    np.save('my_tracks_all', tracks_all)\n",
    "else:\n",
    "    print('load tracks')\n",
    "    tracks_all = np.load('../input/trackml/my_tracks_all.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "ce9865fb97fd16066e062bc375027cb405cf1f51"
   },
   "outputs": [],
   "source": [
    "def get_track_score(tracks_all, n=4):\n",
    "    scores = np.zeros(len(tracks_all))\n",
    "    for i, path in enumerate(tracks_all):\n",
    "        count = len(path)\n",
    "\n",
    "        if count > 1:\n",
    "            tp=0\n",
    "            fp=0\n",
    "            for p in path:\n",
    "                tp = tp + np.sum(np.isin(tracks_all[p], path, assume_unique=True))\n",
    "                fp = fp + np.sum(np.isin(tracks_all[p], path, assume_unique=True, invert=True))\n",
    "            scores[i] = (tp-fp*n-count)/count/(count-1)\n",
    "        else:\n",
    "            scores[i] = -np.inf\n",
    "    return scores\n",
    "\n",
    "def score_event_fast(truth, submission):\n",
    "    truth = truth[['hit_id', 'particle_id', 'weight']].merge(submission, how='left', on='hit_id')\n",
    "    df = truth.groupby(['track_id', 'particle_id']).hit_id.count().to_frame('count_both').reset_index()\n",
    "    truth = truth.merge(df, how='left', on=['track_id', 'particle_id'])\n",
    "    \n",
    "    df1 = df.groupby(['particle_id']).count_both.sum().to_frame('count_particle').reset_index()\n",
    "    truth = truth.merge(df1, how='left', on='particle_id')\n",
    "    df1 = df.groupby(['track_id']).count_both.sum().to_frame('count_track').reset_index()\n",
    "    truth = truth.merge(df1, how='left', on='track_id')\n",
    "    truth.count_both *= 2\n",
    "    score = truth[(truth.count_both > truth.count_particle) & (truth.count_both > truth.count_track)].weight.sum()\n",
    "    particles = truth[(truth.count_both > truth.count_particle) & (truth.count_both > truth.count_track)].particle_id.unique()\n",
    "\n",
    "    return score, truth[truth.particle_id.isin(particles)].weight.sum(), 1-truth[truth.track_id>0].weight.sum()\n",
    "\n",
    "def evaluate_tracks(tracks, truth):\n",
    "    submission = pd.DataFrame({'hit_id': truth.hit_id, 'track_id': tracks})\n",
    "    score = score_event_fast(truth, submission)[0]\n",
    "    track_id = tracks.max()\n",
    "    print('%.4f %2.2f %4d %5d %.4f %.4f'%(score, np.sum(tracks>0)/track_id, track_id, np.sum(tracks==0), 1-score-np.sum(truth.weight.values[tracks==0]), np.sum(truth.weight.values[tracks==0])))\n",
    "\n",
    "def extend_path(path, mask, thr, last = False):\n",
    "    a = 0\n",
    "    for p in path[:-1]:\n",
    "        c = get_predict2(p)\n",
    "        if last == False:\n",
    "            mask = (c > thr)*mask\n",
    "        mask[p] = 0\n",
    "        cand = np.where(c>thr)[0]\n",
    "        mask[cand[np.isin(module_id[cand], module_id[path])]]=0\n",
    "        a = (c + a)*mask\n",
    "\n",
    "    while True:\n",
    "        c = get_predict2(path[-1])\n",
    "        if last == False:\n",
    "            mask = (c > thr)*mask\n",
    "        mask[path[-1]] = 0\n",
    "        cand = np.where(c>thr)[0]\n",
    "        mask[cand[np.isin(module_id[cand], module_id[path])]]=0\n",
    "        a = (c + a)*mask\n",
    "            \n",
    "        if a.max() < thr*len(path):\n",
    "            break\n",
    "\n",
    "        path.append(a.argmax())\n",
    "        if last: break\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "d4670b7668b14f70b2c64727ae48ff319e2c4552"
   },
   "outputs": [],
   "source": [
    "# calculate track's confidence (about 2 mins)\n",
    "scores = get_track_score(tracks_all, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d5bde61fd95aba5e09536d81360003742857062"
   },
   "outputs": [],
   "source": [
    "# merge tracks by confidence and get score\n",
    "idx = np.argsort(scores)[::-1]\n",
    "tracks = np.zeros(len(hits))\n",
    "track_id = 0\n",
    "\n",
    "for hit in idx:\n",
    "\n",
    "    path = np.array(tracks_all[hit])\n",
    "    path = path[np.where(tracks[path]==0)[0]]\n",
    "\n",
    "    if len(path)>3:\n",
    "        track_id = track_id + 1  \n",
    "        tracks[path] = track_id\n",
    "\n",
    "evaluate_tracks(tracks, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "caa0cbbee67c1b4a6eb749d495f865436f3b25eb"
   },
   "outputs": [],
   "source": [
    "# multistage\n",
    "idx = np.argsort(scores)[::-1]\n",
    "tracks = np.zeros(len(hits))\n",
    "track_id = 0\n",
    "\n",
    "for hit in idx:\n",
    "    path = np.array(tracks_all[hit])\n",
    "    path = path[np.where(tracks[path]==0)[0]]\n",
    "\n",
    "    if len(path)>6:\n",
    "        track_id = track_id + 1  \n",
    "        tracks[path] = track_id\n",
    "\n",
    "evaluate_tracks(tracks, truth)\n",
    "\n",
    "for track_id in range(1, int(tracks.max())+1):\n",
    "    path = np.where(tracks == track_id)[0]\n",
    "    path = extend_path(path.tolist(), 1*(tracks==0), 0.6)\n",
    "    tracks[path] = track_id\n",
    "        \n",
    "evaluate_tracks(tracks, truth)\n",
    "        \n",
    "for hit in idx:\n",
    "    path = np.array(tracks_all[hit])\n",
    "    path = path[np.where(tracks[path]==0)[0]]\n",
    "\n",
    "    if len(path)>3:\n",
    "        path = extend_path(path.tolist(), 1*(tracks==0), 0.6)\n",
    "        track_id = track_id + 1  \n",
    "        tracks[path] = track_id\n",
    "        \n",
    "evaluate_tracks(tracks, truth)\n",
    "\n",
    "for track_id in range(1, int(tracks.max())+1):\n",
    "    path = np.where(tracks == track_id)[0]\n",
    "    path = extend_path(path.tolist(), 1*(tracks==0), 0.5)\n",
    "    tracks[path] = track_id\n",
    "        \n",
    "evaluate_tracks(tracks, truth)\n",
    "\n",
    "for hit in idx:\n",
    "    path = np.array(tracks_all[hit])\n",
    "    path = path[np.where(tracks[path]==0)[0]]\n",
    "\n",
    "    if len(path)>1:\n",
    "        path = extend_path(path.tolist(), 1*(tracks==0), 0.5)\n",
    "    if len(path)>2:\n",
    "        track_id = track_id + 1\n",
    "        tracks[path] = track_id\n",
    "        \n",
    "evaluate_tracks(tracks, truth)\n",
    "\n",
    "for track_id in range(1, int(tracks.max())+1):\n",
    "    path = np.where(tracks== track_id)[0]\n",
    "    if len(path)%2 == 0:\n",
    "        path = extend_path(path.tolist(), 1*(tracks==0), 0.5, True)\n",
    "        tracks[path] = track_id\n",
    "        \n",
    "evaluate_tracks(tracks, truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "64f8a293a76cac80636c80a4adb8fe994fc8cef4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
